# âš™ï¸ ë°±ì—”ë“œ êµ¬í˜„ ìƒì„¸

## ğŸ—ï¸ FastAPI ê¸°ë°˜ ë°±ì—”ë“œ

### í•µì‹¬ êµ¬í˜„ ì² í•™
- **ë¹„ë™ê¸° ì²˜ë¦¬**: ë†’ì€ ë™ì‹œì„±ê³¼ ì„±ëŠ¥
- **íƒ€ì… ì•ˆì „ì„±**: Pydanticì„ í†µí•œ ë°ì´í„° ê²€ì¦
- **ëª¨ë“ˆí™”**: ì—­í• ë³„ íŒŒì¼ ë¶„ë¦¬
- **ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”**: ì‹¤ì‹œê°„ ì‘ë‹µ ì²˜ë¦¬

## ğŸ“ íŒŒì¼ êµ¬ì¡° ë° ì—­í• 

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py          # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
â”‚   â”œâ”€â”€ models.py        # Pydantic ë°ì´í„° ëª¨ë¸
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ chat.py      # ì±„íŒ… API ì—”ë“œí¬ì¸íŠ¸
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ ai_service.py # OpenAI API ì—°ë™ ì„œë¹„ìŠ¤
â”œâ”€â”€ requirements.txt     # Python ì˜ì¡´ì„±
â””â”€â”€ .env                # í™˜ê²½ ë³€ìˆ˜ (API í‚¤ ë“±)
```

## ğŸš€ ì£¼ìš” êµ¬í˜„ ë‚´ìš©

### 1. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (`main.py`)

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
from .routers import chat

load_dotenv()  # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ

app = FastAPI(title="AI Chat Service", version="1.0.0")

# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì • - React ì•±ê³¼ì˜ í†µì‹  í—ˆìš©
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì±„íŒ… ë¼ìš°í„° ë“±ë¡
app.include_router(chat.router)

# í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    return {"message": "AI Chat Service API"}

@app.get("/health")
async def health_check():
    return {"status": "healthy"}
```

**í•µì‹¬ í¬ì¸íŠ¸:**
- `load_dotenv()`ë¡œ í™˜ê²½ë³€ìˆ˜ ì•ˆì „í•˜ê²Œ ë¡œë“œ
- CORS ì„¤ì •ìœ¼ë¡œ í”„ë¡ íŠ¸ì—”ë“œì™€ì˜ í†µì‹  í—ˆìš©
- í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ë¡œ ì„œë²„ ìƒíƒœ í™•ì¸ ê°€ëŠ¥

### 2. ë°ì´í„° ëª¨ë¸ (`models.py`)

```python
from pydantic import BaseModel
from typing import List, Optional

class ChatMessage(BaseModel):
    role: str          # 'user' ë˜ëŠ” 'assistant'
    content: str       # ë©”ì‹œì§€ ë‚´ìš©

class ChatRequest(BaseModel):
    message: str                           # ì‚¬ìš©ì ë©”ì‹œì§€
    history: Optional[List[ChatMessage]] = []  # ëŒ€í™” ê¸°ë¡
```

**í•µì‹¬ í¬ì¸íŠ¸:**
- Pydanticìœ¼ë¡œ ë°ì´í„° ê²€ì¦ ìë™í™”
- Optionalë¡œ ê¸°ë³¸ê°’ ì„¤ì •
- íƒ€ì… íŒíŠ¸ë¡œ IDE ì§€ì› í–¥ìƒ

### 3. AI ì„œë¹„ìŠ¤ (`services/ai_service.py`)

```python
import os
from openai import OpenAI
from typing import Generator, List
from ..models import ChatMessage

class AIService:
    def __init__(self):
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    def stream_chat(self, message: str, history: List[ChatMessage] = []) -> Generator[str, None, None]:
        # ëŒ€í™” ê¸°ë¡ì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        messages = []
        for msg in history:
            messages.append({"role": msg.role, "content": msg.content})
        
        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages.append({"role": "user", "content": message})
        
        try:
            # OpenAI ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                stream=True,              # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
                max_tokens=1000,
                temperature=0.7
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            for chunk in response:
                if chunk.choices[0].delta.content is not None:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            yield f"Error: {str(e)}"
```

**í•µì‹¬ í¬ì¸íŠ¸:**
- Generatorë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„
- ëŒ€í™” ê¸°ë¡ ìœ ì§€ë¡œ ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´
- ì˜ˆì™¸ ì²˜ë¦¬ë¡œ ì•ˆì •ì„± í™•ë³´

### 4. ì±„íŒ… ë¼ìš°í„° (`routers/chat.py`)

```python
from fastapi import APIRouter, HTTPException, Query
from fastapi.responses import StreamingResponse
from ..models import ChatRequest, ChatMessage
from ..services.ai_service import AIService
import json
from typing import List, Optional

router = APIRouter(prefix="/api/chat", tags=["chat"])
ai_service = AIService()

@router.post("/stream")
async def stream_chat(request: ChatRequest):
    """POST ìš”ì²­ìœ¼ë¡œ ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë°"""
    try:
        def generate():
            for chunk in ai_service.stream_chat(request.message, request.history):
                # SSE í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì „ì†¡
                yield f"data: {json.dumps({'chunk': chunk})}\n\n"
            # ì™„ë£Œ ì‹ í˜¸
            yield f"data: {json.dumps({'done': True})}\n\n"
        
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*"
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/stream")
async def stream_chat_get(
    message: str = Query(...),
    history: str = Query("[]")
):
    """GET ìš”ì²­ìœ¼ë¡œ ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° (EventSource ìš©)"""
    try:
        # JSON ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ íˆìŠ¤í† ë¦¬ ë³µì›
        history_list = []
        if history != "[]":
            history_data = json.loads(history)
            history_list = [ChatMessage(**item) for item in history_data]
        
        def generate():
            for chunk in ai_service.stream_chat(message, history_list):
                yield f"data: {json.dumps({'chunk': chunk})}\n\n"
            yield f"data: {json.dumps({'done': True})}\n\n"
        
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "*"
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**í•µì‹¬ í¬ì¸íŠ¸:**
- POST/GET ë‘ ê°€ì§€ ë°©ì‹ ì§€ì›
- SSE í‘œì¤€ í—¤ë” ì„¤ì •
- JSON í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ì‘ë‹µ
- ì™„ë£Œ ì‹ í˜¸ë¡œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì•Œë¦¼

## ğŸ”„ Server-Sent Events (SSE) êµ¬í˜„

### SSE ì‘ë‹µ í˜•ì‹
```
data: {"chunk": "ì•ˆë…•"}\n\n
data: {"chunk": "í•˜ì„¸ìš”"}\n\n  
data: {"chunk": "!"}\n\n
data: {"done": true}\n\n
```

### ìŠ¤íŠ¸ë¦¬ë° í—¤ë” ì„¤ì •
```python
headers = {
    "Cache-Control": "no-cache",      # ìºì‹œ ë¹„í™œì„±í™”
    "Connection": "keep-alive",       # ì—°ê²° ìœ ì§€
    "Access-Control-Allow-Origin": "*",     # CORS
    "Access-Control-Allow-Headers": "*"
}
```

## ğŸ›¡ï¸ ì—ëŸ¬ ì²˜ë¦¬ ë° ë³´ì•ˆ

### 1. í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬
```bash
# .env íŒŒì¼
OPENAI_API_KEY=sk-your-api-key-here
```

### 2. ì˜ˆì™¸ ì²˜ë¦¬
```python
try:
    # OpenAI API í˜¸ì¶œ
    response = self.client.chat.completions.create(...)
except Exception as e:
    # í´ë¼ì´ì–¸íŠ¸ì— ì—ëŸ¬ ë©”ì‹œì§€ ì „ì†¡
    yield f"Error: {str(e)}"
```

### 3. CORS ë³´ì•ˆ
- íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
- ê°œë°œ/í”„ë¡œë•ì…˜ í™˜ê²½ë³„ ì„¤ì • ë¶„ë¦¬

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### 1. ë¹„ë™ê¸° ì²˜ë¦¬
```python
# FastAPIì˜ ë¹„ë™ê¸° ê¸°ëŠ¥ í™œìš©
async def stream_chat(request: ChatRequest):
    # ë…¼ë¸”ë¡œí‚¹ ì²˜ë¦¬
```

### 2. ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”
- í† í° ë‹¨ìœ„ ì¦‰ì‹œ ì „ì†¡
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
- ë„¤íŠ¸ì›Œí¬ ì§€ì—° ìµœì†Œí™”

### 3. ì—°ê²° ê´€ë¦¬
- Keep-Aliveë¡œ ì—°ê²° ì¬ì‚¬ìš©
- ì ì ˆí•œ íƒ€ì„ì•„ì›ƒ ì„¤ì •

## ğŸš€ ì‹¤í–‰ ë°©ë²•

```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ì„œë²„ ì‹¤í–‰ (ê°œë°œ ëª¨ë“œ)
uvicorn app.main:app --reload --port 8000

# í”„ë¡œë•ì…˜ ì‹¤í–‰
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

## ğŸ” API ë¬¸ì„œ

FastAPI ìë™ ìƒì„± ë¬¸ì„œ:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc